\relax 
\providecommand \babel@aux [2]{\global \let \babel@toc \@gobbletwo }
\@nameuse{bbl@beforestart}
\catcode `:\active 
\catcode `;\active 
\catcode `!\active 
\catcode `?\active 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\babel@aux{english}{}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{logo_UD}{{\caption@xref {logo_UD}{ on input line 10}}{1}{}{figure.caption.1}{}}
\pgfsyspdfmark {pgfid1}{6749099}{47583362}
\@writefile{toc}{\contentsline {chapter}{List of Figures}{}{chapter*.4}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{List of Tables}{}{chapter*.5}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{List of Abbreviations }{}{chapter*.6}\protected@file@percent }
\citation{HamoudaSidhoum2024}
\citation{Naveed2023}
\citation{lewis2020retrieval}
\citation{Information_Retrieval}
\citation{aisera2024agentic}
\@writefile{toc}{\contentsline {chapter}{General introduction}{1}{chapter*.7}\protected@file@percent }
\citation{BengioDVJ03}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Large Language Models (LLMs) In Agentic AI }{4}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Introduction}{4}{section.1.1}\protected@file@percent }
\newlabel{start1}{{1.1}{4}{Introduction}{section.1.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Language Models}{4}{section.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}Definition of Language Models}{4}{subsection.1.2.1}\protected@file@percent }
\citation{chen1999empirical}
\citation{chen1999empirical}
\citation{jurafsky2000speech}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}N-gram Language Models}{5}{subsection.1.2.2}\protected@file@percent }
\citation{cho2014learning}
\citation{hochreiter1997long}
\citation{BengioDVJ03}
\citation{jurafsky2019speech}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Neural Language Models}{6}{section.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.1}Definition of Neural Language Models}{6}{subsection.1.3.1}\protected@file@percent }
\citation{barnard2024word}
\citation{mikolov2013efficient}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces  Neural language model architecture. . }}{7}{figure.caption.13}\protected@file@percent }
\newlabel{neurel_languge}{{1.1}{7}{Neural language model architecture. }{figure.caption.13}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.2}Word Representations (Embedding)}{7}{subsection.1.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.3}Continuous Bag-of-Words (CBOW)}{7}{subsection.1.3.3}\protected@file@percent }
\citation{pennington2014glove}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.4}Continuous Skip-gram}{8}{subsection.1.3.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces The two Word2Vec architectures: the CBOW model and Skip-gram model.}}{8}{figure.caption.14}\protected@file@percent }
\newlabel{word2vec-architectures}{{1.2}{8}{The two Word2Vec architectures: the CBOW model and Skip-gram model}{figure.caption.14}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.5}Global Vectors for Word Representation(GloVe)}{8}{subsection.1.3.5}\protected@file@percent }
\citation{vaswani2017attention}
\citation{brown2020language,touvron2023llama}
\citation{brown2020language}
\citation{kaplan2020scaling}
\citation{2402-06853}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Large Language Models (LLMs)}{9}{section.1.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.3}{\ignorespaces Cumulative growth of arXiv papers mentioning ``language model'' and ``large language model,'' highlighting the sharp increase after the release of ChatGPT.}}{9}{figure.caption.15}\protected@file@percent }
\newlabel{fig:llm_growth}{{1.3}{9}{Cumulative growth of arXiv papers mentioning ``language model'' and ``large language model,'' highlighting the sharp increase after the release of ChatGPT}{figure.caption.15}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.1}Definition of LLMs}{9}{subsection.1.4.1}\protected@file@percent }
\citation{vaswani2017attention}
\citation{vaswani2017attention}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.2}Historical Development of LLMs}{10}{subsection.1.4.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.4}{\ignorespaces  History and development of language models.}}{10}{figure.caption.16}\protected@file@percent }
\newlabel{History.png}{{1.4}{10}{History and development of language models}{figure.caption.16}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.3}Transformer Architecture}{10}{subsection.1.4.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.5}{\ignorespaces The Transformer - model architecture.}}{11}{figure.caption.17}\protected@file@percent }
\newlabel{trasformers.png}{{1.5}{11}{The Transformer - model architecture}{figure.caption.17}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.4.3.1}Encoder and Decoder Stacks}{11}{subsubsection.1.4.3.1}\protected@file@percent }
\citation{choromanski2020rethinking}
\citation{vaswani2017attention}
\citation{choromanski2020rethinking}
\citation{rothman2021transformers}
\@writefile{lof}{\contentsline {figure}{\numberline {1.6}{\ignorespaces Encoder and Decoder Stacks}}{12}{figure.caption.18}\protected@file@percent }
\newlabel{incoderDecoder.png}{{1.6}{12}{Encoder and Decoder Stacks}{figure.caption.18}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.4.3.2}Self-Attention Mechanism in Transformers}{12}{subsubsection.1.4.3.2}\protected@file@percent }
\citation{rothman2021transformers}
\citation{rothman2021transformers}
\citation{clark2019what}
\citation{vaswani2017attention}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.4.3.3}Positional Encoding:}{13}{subsubsection.1.4.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.4.3.4}Role of Multi-Head Attention}{13}{subsubsection.1.4.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.4}Key Contributions of Transformer in Modern NLP}{13}{subsection.1.4.4}\protected@file@percent }
\citation{devlin2018bert}
\citation{radford2019language}
\citation{openlm2023survey}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.5}Classification of LLMs}{14}{subsection.1.4.5}\protected@file@percent }
\gdef \LT@i {\LT@entry 
    {1}{34.71687pt}\LT@entry 
    {3}{72.20207pt}\LT@entry 
    {3}{31.7564pt}\LT@entry 
    {3}{57.5251pt}\LT@entry 
    {1}{44.53271pt}\LT@entry 
    {1}{71.21231pt}\LT@entry 
    {1}{59.76622pt}\LT@entry 
    {1}{74.35155pt}\LT@entry 
    {1}{95.96182pt}\LT@entry 
    {1}{92.55266pt}}
\@writefile{lot}{\contentsline {table}{\numberline {1.1}{\ignorespaces Selected Large Language Models with key attributes.}}{15}{table.1.1}\protected@file@percent }
\newlabel{llm_table_final}{{1.1}{15}{Classification of LLMs}{table.1.1}{}}
\citation{jurafsky2000speech}
\citation{jurafsky2000speech}
\citation{antlm2024}
\citation{kaplan2020scaling}
\citation{kaplan2020scaling}
\citation{hoffmann2022training}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.6}Training Large Language Models}{16}{subsection.1.4.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.7}Training Objectives and Loss Functions}{16}{subsection.1.4.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.8}Parameter Scaling and Model Size}{16}{subsection.1.4.8}\protected@file@percent }
\citation{wang2023language}
\citation{dong2019unified}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.9}Training Paradigms: Pre-training and Fine-tuning}{17}{subsection.1.4.9}\protected@file@percent }
\citation{touvron2023llama}
\citation{brown2020language}
\@writefile{lof}{\contentsline {figure}{\numberline {1.7}{\ignorespaces Training Tokenization in Full, Prefix, and Masked Language Modeling.}}{18}{figure.caption.20}\protected@file@percent }
\citation{Naveed2023}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.10}Few-Shot, One-Shot, and Zero-Shot Learning}{19}{subsection.1.4.10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.11}Evaluation Datasets}{19}{subsection.1.4.11}\protected@file@percent }
\citation{yenduri2023gpt}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.12}Popular Models}{20}{subsection.1.4.12}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.4.12.1}GPT-N Models}{20}{subsubsection.1.4.12.1}\protected@file@percent }
\citation{devlin2019bert}
\citation{raffel2023exploring}
\citation{touvron2023llama2openfoundation}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.4.12.2}Bidirectional Encoder Representations from Transformers (BERT)}{21}{subsubsection.1.4.12.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.4.12.3}Text-To-Text Transfer Transformer (T5)}{21}{subsubsection.1.4.12.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.4.12.4}LLAMA2 model}{21}{subsubsection.1.4.12.4}\protected@file@percent }
\citation{sengupta2023jais}
\citation{Bender2021}
\citation{Naveed2023}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.4.12.5}Jais and Jais-chat models}{22}{subsubsection.1.4.12.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.13}Limitations of LLMs}{22}{subsection.1.4.13}\protected@file@percent }
\citation{chalkidis2023multilegalbert}
\citation{NiklausMSCH24}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.14}State of the Art in Juridical Data}{23}{subsection.1.4.14}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.5}Agentic AI}{23}{section.1.5}\protected@file@percent }
\citation{aisera2024agentic}
\citation{vectorize2025agentarchitectures}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5.1}Definition of Agentic AI}{24}{subsection.1.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5.2}Architecture Overview of Agentic AI}{24}{subsection.1.5.2}\protected@file@percent }
\citation{google2025aiagent}
\@writefile{lof}{\contentsline {figure}{\numberline {1.8}{\ignorespaces General architecture of an AI agent system. }}{25}{figure.caption.22}\protected@file@percent }
\newlabel{architecture}{{1.8}{25}{General architecture of an AI agent system}{figure.caption.22}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5.3}Types of Agentic AI}{25}{subsection.1.5.3}\protected@file@percent }
\citation{aisera2024agentic}
\@writefile{lof}{\contentsline {figure}{\numberline {1.9}{\ignorespaces  A single-agent system architecture . }}{26}{figure.caption.24}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.10}{\ignorespaces  A multi-agent system architecture. }}{26}{figure.caption.25}\protected@file@percent }
\newlabel{multi_agent}{{1.10}{26}{A multi-agent system architecture}{figure.caption.25}{}}
\citation{touvron2023llama2openfoundation}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5.4} Agent Categories}{27}{subsection.1.5.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5.5}Key Challenges in Agentic AI}{27}{subsection.1.5.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.6}Conclusion}{28}{section.1.6}\protected@file@percent }
\citation{selvaraj2024}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Retrieval-Augmented Generation in Recommendation System}{29}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Introduction}{29}{section.2.1}\protected@file@percent }
\newlabel{start 1}{{2.1}{29}{Introduction}{section.2.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Retrieval-Augmented Generation (RAG)}{29}{section.2.2}\protected@file@percent }
\citation{lewis2020retrieval}
\citation{ibm2024}
\citation{gao2024retrieval}
\citation{gao2024retrieval}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Definition of RAG}{30}{subsection.2.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Historical Development of RAG}{30}{subsection.2.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Comparison with Fine-Tuning and Transfer Learning}{30}{subsection.2.2.3}\protected@file@percent }
\citation{howard2018universal}
\citation{devlin2018bert}
\citation{zhao2024retrieval}
\citation{karpukhin2020dense}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.4}Retrieval Mechanism}{31}{subsection.2.2.4}\protected@file@percent }
\citation{gupta2024comprehensive}
\citation{mombaerts2024meta}
\citation{mombaerts2024meta}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.5}Generation Process}{32}{subsection.2.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.6}Augmentation Techniques}{32}{subsection.2.2.6}\protected@file@percent }
\citation{gao2024retrieval,ahmed2024agenticrag}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Retrieval-Augmented Generation architecture}}{33}{figure.caption.26}\protected@file@percent }
\newlabel{rag_architectur.png}{{2.1}{33}{Retrieval-Augmented Generation architecture}{figure.caption.26}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.7}Types of RAG Systems}{33}{subsection.2.2.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.8}Naive RAG}{33}{subsection.2.2.8}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Types of RAG Systems}}{34}{figure.caption.27}\protected@file@percent }
\newlabel{rag_archi}{{2.2}{34}{Types of RAG Systems}{figure.caption.27}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.9}Advanced RAG}{34}{subsection.2.2.9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.10}Modular RAG}{35}{subsection.2.2.10}\protected@file@percent }
\citation{ahmed2024agenticrag}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.11}Agentic RAG}{36}{subsection.2.2.11}\protected@file@percent }
\citation{zhou2020trustworthiness}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces High-level architecture of an Agentic RAG system.}}{37}{figure.caption.28}\protected@file@percent }
\newlabel{rag_agentic}{{2.3}{37}{High-level architecture of an Agentic RAG system}{figure.caption.28}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.12}Evaluation Methods}{37}{subsection.2.2.12}\protected@file@percent }
\citation{zhao2024retrieval,gupta2024comprehensive}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.13}Challenges and Limitations}{38}{subsection.2.2.13}\protected@file@percent }
\citation{lexemoRAG}
\citation{arabicrag2024}
\citation{legalrag2024}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.14}State of the Art in Juridical Data}{39}{subsection.2.2.14}\protected@file@percent }
\citation{arablegaleval2024}
\citation{Roy2022}
\citation{maruti_recsys}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Recommendation Systems}{40}{section.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Definition of Recommendation Systems}{40}{subsection.2.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Recommendation System Process}}{41}{figure.caption.29}\protected@file@percent }
\newlabel{Recommendation_System _Process}{{2.4}{41}{Recommendation System Process}{figure.caption.29}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Types of Recommendation Systems}{41}{subsection.2.3.2}\protected@file@percent }
\citation{Tamm_2021}
\citation{Tamm_2021}
\citation{Tamm_2021}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Evaluation Metrics}{42}{subsection.2.3.3}\protected@file@percent }
\citation{DiPalma}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.4}Retrieval-Augmented Generation in Recommendation Systems}{43}{subsection.2.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Conclusion}{43}{section.2.4}\protected@file@percent }
\citation{pareto2024rag}
\citation{Rossi_2024}
\citation{geeksforgeeks2022precision}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}k-Selection Optimization in RAG }{44}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Introduction}{44}{section.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Defining k: The Number of Retrieved Documents}{44}{section.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Impact of k on Retrieval Performance}{44}{section.3.3}\protected@file@percent }
\citation{deconvoluteai2024metrics}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Basic retrieval}}{45}{figure.caption.30}\protected@file@percent }
\newlabel{rag_retrival.png}{{3.1}{45}{Basic retrieval}{figure.caption.30}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Recall vs. Precision}{45}{subsection.3.3.1}\protected@file@percent }
\citation{deconvoluteai2024metrics}
\citation{manning2008ir}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Retrieval Speed and Computational Cost}{46}{subsection.3.3.2}\protected@file@percent }
\citation{enwiki:1262179867}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.3}Document Ranking Quality}{47}{subsection.3.3.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Precision in Document Ranking}}{47}{figure.caption.31}\protected@file@percent }
\newlabel{precisionR}{{3.2}{47}{Precision in Document Ranking}{figure.caption.31}{}}
\citation{salemi2023evaluating}
\citation{wan2025cognitivealigneddocumentselectionretrievalaugmented}
\citation{freitag-al-onaizan-2017-beam}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Recall in Document Ranking}}{48}{figure.caption.32}\protected@file@percent }
\newlabel{recalR}{{3.3}{48}{Recall in Document Ranking}{figure.caption.32}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Impact of k on Generation Quality}{48}{section.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}Trade-off Between Diversity and Relevance}{48}{subsection.3.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.2}Effect on Text Generation Models}{48}{subsection.3.4.2}\protected@file@percent }
\citation{tensorrt_llm_beam_search}
\citation{zheng2024enhancing}
\citation{10.1561/1500000019}
\citation{gfg2025tfidf}
\citation{10.1007/978-3-030-72240-1_49}
\citation{karpukhin2020dense}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Existing Solutions for k Selection}{49}{section.3.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.1}Static k Selection}{49}{subsection.3.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.5.1.1}Sparse Retrieval with Fixed k}{49}{subsubsection.3.5.1.1}\protected@file@percent }
\citation{chen-etal-2024-dense}
\citation{enwiki:1276232158}
\citation{culpepper2016dynamictradeoffpredictionmultistage}
\citation{10.1007/978-3-319-70145-5_1}
\citation{sawarkar2024blendedragimprovingrag}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.5.1.2}Dense Retrieval with Fixed k}{50}{subsubsection.3.5.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.2}Dynamic k Selection}{50}{subsection.3.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.5.2.1}Dynamic Trade-Off Prediction in Multi-Stage Retrieval Systems }{50}{subsubsection.3.5.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.5.2.2}Dynamic Pruning Methods:}{50}{subsubsection.3.5.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.3}Hybrid k Selection}{50}{subsection.3.5.3}\protected@file@percent }
\citation{zhu2024staykatehybridincontextexample}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.5.3.1}Blended RAG}{51}{subsubsection.3.5.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.5.3.2} STAYKATE (Static-Dynamic Hybrid Selection)}{51}{subsubsection.3.5.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.6}Proposed Solution}{51}{section.3.6}\protected@file@percent }
\citation{zhai2023revisiting,Ding2024}
\citation{zhai2023revisiting}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces Notation Table }}{52}{table.caption.33}\protected@file@percent }
\newlabel{tab:Notations_table}{{3.1}{52}{Notation Table}{table.caption.33}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.1}Mixture of Logits (MoL)}{52}{subsection.3.6.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Mixture of Logits(MoL) learned similarity.}}{52}{figure.caption.34}\protected@file@percent }
\newlabel{Mixture_of_Logits }{{3.4}{52}{Mixture of Logits(MoL) learned similarity}{figure.caption.34}{}}
\citation{zhai2023revisiting}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.2}Algorithm Design}{53}{subsection.3.6.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces propused solution steps}}{53}{figure.caption.35}\protected@file@percent }
\newlabel{propused _solution_steps}{{3.5}{53}{propused solution steps}{figure.caption.35}{}}
\newlabel{eq:initial_retrieval}{{3.8}{54}{Algorithm Design}{equation.3.8}{}}
\newlabel{similarity_function}{{3.9}{54}{Algorithm Design}{equation.3.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.3}Hybrid k-Selection Algorithm Pseudocode}{54}{subsection.3.6.3}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Hybrid Exact Top-k with Threshold-Based k Selection}}{55}{algorithm.1}\protected@file@percent }
\newlabel{ALgo1}{{1}{55}{Hybrid Exact Top-k with Threshold-Based k Selection}{algorithm.1}{}}
\citation{kang2018selfat}
\citation{Harper2015}
\@writefile{toc}{\contentsline {section}{\numberline {3.7}Experimental Results}{56}{section.3.7}\protected@file@percent }
\newlabel{sec3}{{3.7}{56}{Experimental Results}{section.3.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7.1}Dataset Design}{56}{subsection.3.7.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3.2}{\ignorespaces Summary of MovieLens datasets}}{56}{table.caption.36}\protected@file@percent }
\newlabel{tab_movielens}{{3.2}{56}{Summary of MovieLens datasets}{table.caption.36}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7.2}Experimental Setup }{56}{subsection.3.7.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.7.2.1}Fixed Experimental Configuration}{57}{subsubsection.3.7.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7.3}Architectural Variants and Results}{57}{subsection.3.7.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3.3}{\ignorespaces Performance Across Configurations}}{57}{table.caption.37}\protected@file@percent }
\newlabel{tab:all_results}{{3.3}{57}{Performance Across Configurations}{table.caption.37}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7.4}Impact of Adaptive k-Variations on Model Performance}{58}{subsection.3.7.4}\protected@file@percent }
\newlabel{fig:k_changes_100K}{{3.6a}{58}{K changes across epochs for MovieLens 100K}{figure.caption.38}{}}
\newlabel{sub@fig:k_changes_100K}{{a}{58}{K changes across epochs for MovieLens 100K}{figure.caption.38}{}}
\newlabel{fig:k_changes_1M}{{3.6b}{58}{K changes across epochs for MovieLens 1M}{figure.caption.38}{}}
\newlabel{sub@fig:k_changes_1M}{{b}{58}{K changes across epochs for MovieLens 1M}{figure.caption.38}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces Comparison of K changes across epochs for different datasets}}{58}{figure.caption.38}\protected@file@percent }
\newlabel{fig:k_changes}{{3.6}{58}{Comparison of K changes across epochs for different datasets}{figure.caption.38}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces Comparison of Models Across Evaluation Metrics}}{59}{figure.caption.39}\protected@file@percent }
\newlabel{histo_p}{{3.7}{59}{Comparison of Models Across Evaluation Metrics}{figure.caption.39}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.8}Conclusion}{60}{section.3.8}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Design of Arabic RAG-Based Agent for Legal and Juridical Data.}{61}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Introduction}{61}{section.4.1}\protected@file@percent }
\newlabel{start4}{{4.1}{61}{Introduction}{section.4.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Dataset Design}{61}{section.4.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces Summary of the datasets used in the Arabic Legal RAG Agent}}{62}{table.caption.40}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces The detailed pipeline used for the creation of both the legal case dataset and the question-answer QA}}{63}{figure.caption.41}\protected@file@percent }
\newlabel{dataset_creation}{{4.1}{63}{The detailed pipeline used for the creation of both the legal case dataset and the question-answer QA}{figure.caption.41}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Agent Architecture Overview}{63}{section.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}Input/Output}{63}{subsection.4.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Architecture of the Arabic RAG-Based Agent for Legal Data. The Agent takes Arabic natural-language questions as input and returns answers grounded in real legal case context. It integrates preprocessing, semantic retrieval, and generation agent. }}{64}{figure.caption.42}\protected@file@percent }
\newlabel{agentic_architecture}{{4.2}{64}{Architecture of the Arabic RAG-Based Agent for Legal Data. The Agent takes Arabic natural-language questions as input and returns answers grounded in real legal case context. It integrates preprocessing, semantic retrieval, and generation agent}{figure.caption.42}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.2}Preprocessing and Ingestion Phase}{64}{subsection.4.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.3}Retriever Sub-Agent }{64}{subsection.4.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.4}Recommendation Sub-Agent}{65}{subsection.4.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.5}Context Selector: Token-Aware Filtering}{65}{subsection.4.3.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.6}Functions and Tools}{65}{subsection.4.3.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.7}Generator: Answer Formulation Component}{65}{subsection.4.3.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.8}Agent Design}{65}{subsection.4.3.8}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Training the Agent}{66}{section.4.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces An overview of the training pipeline for the RAG agent }}{66}{figure.caption.43}\protected@file@percent }
\newlabel{agentic_trining}{{4.3}{66}{An overview of the training pipeline for the RAG agent}{figure.caption.43}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.1}Training the Retriever}{66}{subsection.4.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.2}Training the Generator}{68}{subsection.4.4.2}\protected@file@percent }
\citation{papineni2002bleu}
\citation{zhang2019bertscore}
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Experimental Results}{70}{section.4.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.1}Training Results}{70}{subsection.4.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.5.1.1}Model Architecture}{70}{subsubsection.4.5.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.5.1.2}Training Configuration}{70}{subsubsection.4.5.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.5.1.3}Inference Settings}{70}{subsubsection.4.5.1.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4.2}{\ignorespaces  Core architectural specifications of the decoder-only transformer model used in this work}}{71}{table.caption.46}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4.3}{\ignorespaces Summary of key hyperparameters configured for model fine-tuning}}{71}{table.caption.47}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.5.1.4}Training Progress :}{71}{subsubsection.4.5.1.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces Training Loss over Epochs}}{72}{figure.caption.48}\protected@file@percent }
\newlabel{Training}{{4.4}{72}{Training Loss over Epochs}{figure.caption.48}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.2}Evaluation Results}{73}{subsection.4.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.3}Retriever Evaluation Results}{73}{subsection.4.5.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces Retrieval performance metrics across different top-K values. The histogram shows consistent improvement in Recall@K, MRR@K, and Hit@K as K increases.}}{73}{figure.caption.49}\protected@file@percent }
\newlabel{fig:retriever_performance}{{4.5}{73}{Retrieval performance metrics across different top-K values. The histogram shows consistent improvement in Recall@K, MRR@K, and Hit@K as K increases}{figure.caption.49}{}}
\@writefile{toc}{\contentsline {paragraph}{Human Evaluation:}{74}{section*.50}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces Retrieval performance on a sample legal query showing the top-3 results with similarity scores. }}{74}{figure.caption.51}\protected@file@percent }
\newlabel{fig:retrieval_example}{{4.6}{74}{Retrieval performance on a sample legal query showing the top-3 results with similarity scores}{figure.caption.51}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.4}Geerator Evaluation Results }{74}{subsection.4.5.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Results:}{74}{section*.52}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Human Evaluation:}{74}{section*.53}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces An example from the Arabic legal assistant interface showing rejection of an irrelevant question ("What is NLP?") because it is not written in formal Arabic. This reflects the Agent ability to filter non-domain input during inference.}}{75}{figure.caption.54}\protected@file@percent }
\newlabel{fig_human_eval_example}{{4.7}{75}{An example from the Arabic legal assistant interface showing rejection of an irrelevant question ("What is NLP?") because it is not written in formal Arabic. This reflects the Agent ability to filter non-domain input during inference}{figure.caption.54}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.6}Agent Results}{75}{section.4.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.8}{\ignorespaces  Interactive interface of the Intelligent Legal Assistant Agent.}}{76}{figure.caption.55}\protected@file@percent }
\newlabel{inter}{{4.8}{76}{Interactive interface of the Intelligent Legal Assistant Agent}{figure.caption.55}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.9}{\ignorespaces Interface of the Arabic Legal RAG Assistant showing a successful answer to a general legal query using the case title. The Agent retrieves relevant case data and generates a comprehensive legal response .}}{77}{figure.caption.56}\protected@file@percent }
\newlabel{legal_rag_interface}{{4.9}{77}{Interface of the Arabic Legal RAG Assistant showing a successful answer to a general legal query using the case title. The Agent retrieves relevant case data and generates a comprehensive legal response }{figure.caption.56}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.7}Conclusion}{78}{section.4.7}\protected@file@percent }
\bibstyle{plainnat}
\bibdata{Biblio}
\bibcite{ahmed2024agenticrag}{{1}{2024}{{Ahmed}}{{}}}
\bibcite{pareto2024rag}{{2}{2024}{{AI}}{{}}}
\bibcite{aisera2024agentic}{{3}{2024}{{Aisera}}{{}}}
\bibcite{arabicrag2024}{{4}{2025}{{Al-Rasheed et~al.}}{{}}}
\bibcite{barnard2024word}{{5}{2024}{{Barnard}}{{}}}
\bibcite{Bender2021}{{6}{2021}{{Bender et~al.}}{{Bender, Gebru, McMillan-Major, and Shmitchell}}}
\bibcite{BengioDVJ03}{{7}{2003}{{Bengio et~al.}}{{Bengio, Ducharme, Vincent, and Janvin}}}
\bibcite{brown2020language}{{8}{2020}{{Brown et~al.}}{{Brown, Mann, Ryder, et~al.}}}
\bibcite{chalkidis2023multilegalbert}{{9}{2023}{{Chalkidis et~al.}}{{Chalkidis, Jana, Hartung, Bommarito, Androutsopoulos, Katz, and Aletras}}}
\bibcite{chen1999empirical}{{10}{1999}{{Chen and Goodman}}{{}}}
\@writefile{toc}{\contentsline {chapter}{Bibliography}{80}{chapter*.58}\protected@file@percent }
\bibcite{chen-etal-2024-dense}{{11}{2024}{{Chen et~al.}}{{Chen, Wang, Chen, Yu, Ma, Xinran, Zhang, and Yu}}}
\bibcite{cho2014learning}{{12}{2014}{{Cho et~al.}}{{Cho, van Merrienboer, et~al.}}}
\bibcite{choromanski2020rethinking}{{13}{2021}{{Choromanski et~al.}}{{Choromanski, Likhosherstov, and Dohan}}}
\bibcite{2402-06853}{{14}{2024}{{Chu et~al.}}{{}}}
\bibcite{clark2019what}{{15}{2019}{{Clark et~al.}}{{Clark, Khandelwal, Levy, and Manning}}}
\bibcite{enwiki:1276232158}{{16}{2025}{{contributors}}{{}}}
\bibcite{culpepper2016dynamictradeoffpredictionmultistage}{{17}{2016}{{Culpepper et~al.}}{{Culpepper, Clarke, and Lin}}}
\bibcite{devlin2018bert}{{18}{2018}{{Devlin et~al.}}{{Devlin, Chang, Lee, and Toutanova}}}
\bibcite{devlin2019bert}{{19}{2019}{{Devlin et~al.}}{{Devlin, Chang, et~al.}}}
\bibcite{DiPalma}{{20}{2023}{{Di~Palma}}{{}}}
\bibcite{Ding2024}{{21}{2025}{{Ding and Zhai}}{{}}}
\bibcite{dong2019unified}{{22}{2019}{{Dong et~al.}}{{Dong, Yang, Wang, Wei, Liu, Wang, Gao, Zhou, and Hon}}}
\bibcite{freitag-al-onaizan-2017-beam}{{23}{2017}{{Freitag and Al-Onaizan}}{{}}}
\bibcite{gao2024retrieval}{{24}{2024}{{Gao et~al.}}{{Gao, Xiong, Gao, Jia, Pan, Bi, Dai, Sun, Wang, and Wang}}}
\bibcite{geeksforgeeks2022precision}{{25}{2022}{{GeeksforGeeks}}{{}}}
\bibcite{gfg2025tfidf}{{26}{2025}{{GeeksforGeeks}}{{}}}
\bibcite{google2025aiagent}{{27}{2025}{{Google Cloud}}{{}}}
\bibcite{gupta2024comprehensive}{{28}{2024}{{Gupta et~al.}}{{Gupta, Ranjan, and Singh}}}
\bibcite{HamoudaSidhoum2024}{{29}{2024}{{Hamouda~Sidhoum et~al.}}{{Hamouda~Sidhoum, M’hamed, Faouzi, Aymen, Fedoua, Mohamed~Chakib, and Takieddine}}}
\bibcite{Harper2015}{{30}{2016}{{Harper and Konstan}}{{}}}
\bibcite{arablegaleval2024}{{31}{2024}{{Hijazi et~al.}}{{Hijazi, Alharbi, et~al.}}}
\bibcite{hochreiter1997long}{{32}{1997}{{Hochreiter and Schmidhuber}}{{}}}
\bibcite{hoffmann2022training}{{33}{2022}{{Hoffmann et~al.}}{{}}}
\bibcite{howard2018universal}{{34}{2018}{{Howard and Ruder}}{{}}}
\bibcite{jurafsky2000speech}{{35}{2009}{{Jurafsky and Martin}}{{}}}
\bibcite{jurafsky2019speech}{{36}{2019}{{Jurafsky and Martin}}{{}}}
\bibcite{legalrag2024}{{37}{2025}{{Kabir et~al.}}{{Kabir, Sultan, Rahman, Amin, Momen, Mohammed, and Rahman}}}
\bibcite{kang2018selfat}{{38}{2018}{{Kang and McAuley}}{{}}}
\bibcite{kaplan2020scaling}{{39}{2020}{{Kaplan et~al.}}{{Kaplan, McCandlish, et~al.}}}
\bibcite{karpukhin2020dense}{{40}{2020}{{Karpukhin et~al.}}{{Karpukhin,  ~Oguz, Min, Lewis, Wu, Edunov, Chen, and Yih}}}
\bibcite{deconvoluteai2024metrics}{{41}{2024}{{Kirchhoff}}{{}}}
\bibcite{lewis2020retrieval}{{42}{2020}{{Lewis et~al.}}{{Lewis, Perez, Piktus, et~al.}}}
\bibcite{lexemoRAG}{{43}{}{{Lexemo}}{{}}}
\bibcite{manning2008ir}{{44}{2008}{{Manning et~al.}}{{Manning, Raghavan, and Schütze}}}
\bibcite{Information_Retrieval}{{45}{2016}{{Mezghanni and Gargouri}}{{}}}
\bibcite{mikolov2013efficient}{{46}{2013}{{Mikolov et~al.}}{{Mikolov, Chen, Corrado, and Dean}}}
\bibcite{mombaerts2024meta}{{47}{2024}{{Mombaerts et~al.}}{{Mombaerts, Ding, Banerjee, Felice, Taws, and Borogovac}}}
\bibcite{Naveed2023}{{48}{2023}{{Naveed et~al.}}{{}}}
\bibcite{NiklausMSCH24}{{49}{}{{Niklaus et~al.}}{{Niklaus, Matoshi, St{\"{u}}rmer, Chalkidis, and Ho}}}
\bibcite{tensorrt_llm_beam_search}{{50}{2023}{{NVIDIA}}{{}}}
\bibcite{papineni2002bleu}{{51}{2002}{{Papineni et~al.}}{{Papineni, Roukos, Ward, and Zhu}}}
\bibcite{pennington2014glove}{{52}{2014}{{Pennington et~al.}}{{Pennington, Socher, and Manning}}}
\bibcite{radford2019language}{{53}{2019}{{Radford et~al.}}{{Radford, Wu, Child, Luan, Amodei, and Sutskever}}}
\bibcite{raffel2023exploring}{{54}{2023}{{Raffel et~al.}}{{Raffel, Shazeer, Roberts, Lee, Narang, et~al.}}}
\bibcite{ibm2024}{{55}{2024}{{Research}}{{}}}
\bibcite{10.1561/1500000019}{{56}{2009}{{Robertson and Zaragoza}}{{}}}
\bibcite{Rossi_2024}{{57}{2024}{{Rossi et~al.}}{{Rossi, Lin, Liu, Yang, Lee, Magnani, and Liao}}}
\bibcite{rothman2021transformers}{{58}{2021}{{Rothman}}{{}}}
\bibcite{Roy2022}{{59}{2022}{{Roy and Dutta}}{{}}}
\bibcite{salemi2023evaluating}{{60}{2023}{{Salemi and Zamani}}{{}}}
\bibcite{sawarkar2024blendedragimprovingrag}{{61}{2024}{{Sawarkar et~al.}}{{Sawarkar, Mangal, and Solanki}}}
\bibcite{selvaraj2024}{{62}{2024}{{Selvaraj}}{{}}}
\bibcite{sengupta2023jais}{{63}{2023}{{Sengupta et~al.}}{{Sengupta, Sahu, Jia, Katipomu, et~al.}}}
\bibcite{Tamm_2021}{{64}{2021}{{Tamm et~al.}}{{Tamm, Damdinov, and Vasilev}}}
\bibcite{maruti_recsys}{{65}{2017}{{Techlabs}}{{}}}
\bibcite{touvron2023llama}{{66}{2023{a}}{{Touvron et~al.}}{{Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozière, Goyal, Hambro, Azhar, et~al.}}}
\bibcite{touvron2023llama2openfoundation}{{67}{2023{b}}{{Touvron et~al.}}{{Touvron, Martin, Stone, Albert, Almahairi, Babaei, et~al.}}}
\bibcite{vaswani2017attention}{{68}{2017}{{Vaswani et~al.}}{{Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin}}}
\bibcite{vectorize2025agentarchitectures}{{69}{2025}{{Vectorize}}{{}}}
\bibcite{wan2025cognitivealigneddocumentselectionretrievalaugmented}{{70}{2025}{{Wan et~al.}}{{Wan, Zhang, Qi, Ding, Li, Fan, Zhang, and Zhang}}}
\bibcite{wang2023language}{{71}{2022}{{Wang et~al.}}{{Wang, Roberts, Hesslow, Le~Scao, Chung, et~al.}}}
\bibcite{enwiki:1262179867}{{72}{2024}{{Wikipedia contributors}}{{}}}
\bibcite{10.1007/978-3-319-70145-5_1}{{73}{2017}{{Wu et~al.}}{{Wu, Lu, et~al.}}}
\bibcite{yenduri2023gpt}{{74}{2023}{{Yenduri et~al.}}{{Yenduri, Ramalingam, Selvi, Supriya, Srivastava, et~al.}}}
\bibcite{antlm2024}{{75}{2024}{{Yu et~al.}}{{Yu, Guo, et~al.}}}
\bibcite{zhai2023revisiting}{{76}{2023}{{Zhai et~al.}}{{Zhai, Gong, Wang, Sun, Yan, Li, and Liu}}}
\bibcite{zhang2019bertscore}{{77}{2020}{{Zhang et~al.}}{{Zhang, Kishore, Wu, Weinberger, and Artzi}}}
\bibcite{zhao2024retrieval}{{78}{2024}{{Zhao et~al.}}{{Zhao, Zhang, Yu, Wang, Geng, Fu, Yang, Zhang, and Cui}}}
\bibcite{openlm2023survey}{{79}{}{{Zhao et~al.}}{{Zhao, Zhou, et~al.}}}
\bibcite{zheng2024enhancing}{{80}{2024}{{Zheng}}{{}}}
\bibcite{zhou2020trustworthiness}{{81}{2020}{{Zhou et~al.}}{{Zhou, Liu, Li, Jin, Qian, Liu, Li, Dou, Ho, and Yu}}}
\bibcite{zhu2024staykatehybridincontextexample}{{82}{2024}{{Zhu et~al.}}{{Zhu, Shimada, Taniguchi, and Ohkuma}}}
\bibcite{10.1007/978-3-030-72240-1_49}{{83}{2021}{{Zhuang et~al.}}{{Zhuang, Li, and Zuccon}}}
\@input{Appendix_B.aux}
\gdef \@abspage@last{112}
